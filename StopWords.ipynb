{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are words that considered not significant to a text when processing the data. Such words may include:\n",
    "\n",
    "[Determiners (words placed in front of nouns)](https://www.grammar-monster.com/glossary/determiner.htm#:~:text=A%20determiner%20is%20a%20word,%2C%20that%2C%20these%2C%20those):\n",
    "\n",
    "    An Article (a/an, the)\n",
    "    A Demonstrative (this, that, these, those)\n",
    "    A Possessive (my, your, his, her, its, our, their)\n",
    "    A Quantifier (common examples include many, much, more, most, some)\n",
    "[Prepositions (indicate direction, time, location, and spatial relationships)](https://www.grammarly.com/blog/prepositions/?gclid=Cj0KCQiApY6BBhCsARIsAOI_GjacuOmcuc2Whs3mZEn16FeI_P0cK3_0Hq-g2wGZD3Je7t5iE6fPwo0aAtr8EALw_wcB&gclsrc=aw.ds):\n",
    "    \n",
    "    Direction: to, aross, through\n",
    "    Time: since, about, after\n",
    "    Location: at, elsewhere, in front of\n",
    "    Space: under, above, below, inside"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "1. Sklearn\n",
    "2. NLTK\n",
    "3. SPACY\n",
    "\n",
    "This Jupyter Notebook compares three diffrent approaches for using stop wordds. In order to determine why to use one package for another package during text processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKLEARN Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'con', 'could', 'couldnt', 'cry', 'de', 'describe', 'detail', 'do', 'done', 'down', 'due', 'during', 'each', 'eg', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed', 'interest', 'into', 'is', 'it', 'its', 'itself', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 'same', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system', 'take', 'ten', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "list = []\n",
    "\n",
    "s =  set(stop_words.ENGLISH_STOP_WORDS)\n",
    "\n",
    "for x in s:\n",
    "    \n",
    "    list.append(x)\n",
    "    \n",
    "print(sorted(list[:680]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nl = stopwords.words('english')\n",
    "print(sorted(stopwords.words('english')[:680]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare SKLEARN and NLTK Stop Words\n",
    "\n",
    "What is the difference between SKLEARN and NLTK? What words are included in SKLEARN, that are not included in NLTK? and vise versa. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Stop words in SKLEARN that are not in NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['many', 'nowhere', 'enough', 'eg', 'wherein', 'eight', 'around', 'cannot', 'either', 'afterwards', 'would', 'thus', 'see', 'beside', 'inc', 'cant', 'amongst', 'done', 'next', 'moreover', 'co', 'someone', 'please', 'anyhow', 'put', 'thick', 'almost', 'must', 'whose', 'describe', 'yet', 'thereby', 'anything', 'still', 'always', 'third', 'rather', 'well', 'whereas', 'another', 'ten', 'get', 'none', 'mill', 'also', 'among', 'sometime', 'fifteen', 'thereupon', 'fifty', 'latter', 'whether', 'meanwhile', 'serious', 'several', 'move', 'become', 'least', 'nothing', 'something', 'without', 'besides', 'fill', 'side', 'onto', 'whole', 'along', 'hereafter', 'call', 'first', 'alone', 'sometimes', 'sixty', 'four', 'whereafter', 'etc', 'much', 'empty', 'per', 'already', 'upon', 'therefore', 'throughout', 'whereupon', 'name', 'anywhere', 'somehow', 'beforehand', 'forty', 'hundred', 'found', 'others', 'became', 'due', 'bill', 'take', 'full', 'couldnt', 'thin', 'former', 'mine', 'sincere', 'formerly', 'namely', 'one', 'ever', 'seems', 'elsewhere', 'becomes', 'cry', 'hereupon', 'otherwise', 'hasnt', 'latterly', 'de', 'part', 'find', 'even', 'toward', 'hereby', 'often', 'behind', 'everyone', 'via', 'neither', 'indeed', 'beyond', 'nevertheless', 'though', 'give', 'whoever', 'therein', 'amount', 'amoungst', 'twenty', 'whenever', 'nobody', 'system', 'top', 'wherever', 'seeming', 'however', 'bottom', 'keep', 'two', 'seemed', 'whereby', 'last', 'mostly', 'except', 'con', 'us', 'perhaps', 'never', 'nine', 'front', 'go', 'interest', 'within', 'across', 'back', 'less', 'anyway', 'ie', 'three', 'six', 'ltd', 'may', 'noone', 'made', 'might', 'everywhere', 'detail', 'eleven', 'somewhere', 'whatever', 'whither', 'hence', 'together', 'five', 'since', 'twelve', 'seem', 'anyone', 'herein', 'else', 'every', 'un', 'towards', 'thru', 'everything', 'although', 'whence', 'thereafter', 'could', 'thence', 'fire', 'show', 'becoming']\n"
     ]
    }
   ],
   "source": [
    "list_difference = [item for item in s if item not in nl]\n",
    "\n",
    "print(list_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of prepositions that are in SKLEARN or not in NLTK.\n",
    "\n",
    "Some Determiners: Many, anywhere, thru, thereafter, therefore, upon, hereafter, along, alone, somtimes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Stop words in NLTK that are not included in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"you're\", \"you've\", \"you'll\", \"you'd\", \"she's\", \"it's\", 'theirs', \"that'll\", 'having', 'does', 'did', 'doing', 's', 't', 'just', 'don', \"don't\", \"should've\", 'd', 'll', 'm', 'o', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "list_difference = [item for item in nl if item not in s]\n",
    "\n",
    "print(list_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more stop words in SKLEARN, than in NLTK.NLTK Stop words includes more possessive determiners such as you're, she, its, their."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stop words\n",
    "import spacy\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Stop words from spacy\n",
    "spc = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'many', 'own', 'nowhere', 'wherein', 'eight', 'cannot', '’s', 'either', 'afterwards', 'n’t', '’re', 'see', 'was', 'beside', 'each', 'into', 'if', 'someone', 'should', \"'re\", 'who', 'please', 'other', 'anyhow', 'whom', 'or', 'almost', 'whose', 're', 'yet', 'thereby', 'anything', 'still', 'were', 'in', 'rather', \"'ve\", 'well', 'hers', 'whereas', 'say', \"'s\", 'why', 'few', 'sometime', 'fifteen', 'and', 'fifty', 'only', 'doing', 'all', 'serious', 'you', 'several', 'here', 'them', 'something', 'with', 'without', 'besides', 'where', 'whole', 'hereafter', 'call', 'first', 'sometimes', 'the', 'four', 'between', 'much', 'per', 'these', 'further', 'upon', 'too', 'used', 'itself', 'therefore', 'throughout', 'name', 'beforehand', 'being', 'hundred', 'up', 'others', 'down', 'by', 'regarding', 'n‘t', 'take', 'mine', 'than', 'yourself', 'formerly', 'ever', 'seems', 'becomes', 'hereupon', '‘m', 'no', 'otherwise', 'until', 'it', 'yourselves', 'most', 'even', 'so', 'toward', 'hereby', 'often', 'an', 'behind', 'herself', 'this', 'though', 'give', 'whoever', 'therein', 'twenty', 'whenever', 'nobody', '’d', 'top', 'seeming', 'his', 'over', 'bottom', 'keep', 'two', 'which', 'whereby', 'mostly', 'except', 'us', 'never', 'been', 'nine', 'front', 'go', 'across', 'on', \"'ll\", 'three', 'there', 'noone', 'but', 'might', 'eleven', 'somewhere', '‘re', 'whither', 'together', 'yours', 'various', 'seem', 'does', 'off', 'anyone', 'every', 'under', 'thru', 'thereafter', 'those', \"'d\", 'thence', 'i', \"'m\", 'they', \"n't\", 'below', 'enough', 'more', 'at', 'what', 'around', 'would', 'thus', 'ourselves', 'amongst', 'done', 'next', 'moreover', 'have', 'put', 'must', 'using', 'both', 'had', 'she', '’m', 'always', 'third', 'another', 'himself', 'ten', 'can', 'will', 'of', 'get', 'none', 'also', 'among', 'then', 'from', 'again', 'any', 'thereupon', 'its', '‘ll', 'latter', 'to', 'whether', 'meanwhile', 'unless', 'above', 'move', 'become', 'least', 'nothing', 'side', 'onto', 'along', '’ve', 'before', 'alone', 'that', 'sixty', 'whereafter', 'really', 'empty', 'not', 'just', 'already', 'whereupon', 'anywhere', 'her', 'make', 'somehow', 'forty', 'ca', 'against', 'became', 'due', 'when', 'myself', 'for', 'full', 'nor', 'former', 'has', 'such', 'namely', 'one', 'be', 'some', 'elsewhere', 'our', 'latterly', '‘ve', 'he', 'about', 'him', 'part', 'everyone', 'via', 'how', 'are', 'me', 'neither', 'indeed', 'beyond', 'nevertheless', 'do', 'we', 'amount', 'a', 'your', 'is', 'wherever', 'out', 'however', '‘s', 'my', 'did', 'seemed', 'their', 'last', 'quite', 'am', 'perhaps', 'themselves', 'very', 'within', 'after', 'back', 'less', '’ll', 'because', 'anyway', 'six', 'may', '‘d', 'made', 'everywhere', 'whatever', 'hence', 'ours', 'during', 'while', 'five', 'since', 'twelve', 'herein', 'as', 'same', 'else', 'now', 'towards', 'everything', 'although', 'once', 'whence', 'could', 'through', 'show', 'becoming'}\n"
     ]
    }
   ],
   "source": [
    "print(spc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare SKLEARN and Spacy Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Stop words in Spacy that are not in SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop words in spacy that are not in scikit-learn:\n",
      "\n",
      "['’s', 'n’t', '’re', \"'re\", \"'ve\", 'say', \"'s\", 'doing', 'used', 'regarding', 'n‘t', '‘m', '’d', \"'ll\", '‘re', 'various', 'does', \"'d\", \"'m\", \"n't\", 'using', '’m', '‘ll', 'unless', '’ve', 'really', 'just', 'make', 'ca', '‘ve', '‘s', 'did', 'quite', '’ll', '‘d']\n"
     ]
    }
   ],
   "source": [
    "# Stop words in spacy that are not in scikit-learn\n",
    "list_difference = [item for item in spc if item not in s]\n",
    "\n",
    "print('stop words in spacy that are not in scikit-learn:')\n",
    "print('')\n",
    "print(list_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Stop words in SKLEARN that are not in SPACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words in scikit learn that are not in spacy:\n",
      "\n",
      "['eg', 'inc', 'cant', 'co', 'thick', 'describe', 'mill', 'fill', 'etc', 'found', 'bill', 'couldnt', 'thin', 'sincere', 'cry', 'hasnt', 'de', 'find', 'amoungst', 'system', 'con', 'interest', 'ie', 'ltd', 'detail', 'un', 'fire']\n"
     ]
    }
   ],
   "source": [
    "# Stop words in scikit learn that are not in spacy\n",
    "list_difference = [item for item in s if item not in spc]\n",
    "\n",
    "print('Stop words in scikit learn that are not in spacy:')\n",
    "print('')\n",
    "print(list_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy contains more stop words than sklearn, but not a lot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out stop words on twitter docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import june date with sentiment analysis\n",
    "df = pd.read_csv('data/2020-08_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is our last stand folks And here’s your last defender If they take him down America is gone forever Vote for realDonaldTrump like your life depends on it'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['full_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is our last stand folks And here’s your last defender If they take him down America is gone forever Vote for realDonaldTrump like your life depends on it\n",
      "\n",
      "Removing STOP WORDS with NLTK\n",
      "['This', 'is', 'our', 'last', 'stand', 'folks', 'And', 'here', '’', 's', 'your', 'last', 'defender', 'If', 'they', 'take', 'him', 'down', 'America', 'is', 'gone', 'forever', 'Vote', 'for', 'realDonaldTrump', 'like', 'your', 'life', 'depends', 'on', 'it']\n"
     ]
    }
   ],
   "source": [
    "# NLTK\n",
    "\n",
    "doc = df['full_text'][0]\n",
    "tokens = nltk.tokenize.word_tokenize(doc)\n",
    "\n",
    "print(df['full_text'][0])\n",
    "print('')\n",
    "print('Removing STOP WORDS with NLTK')\n",
    "print([word for word in tokens if word is not nl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is our last stand folks And here’s your last defender If they take him down America is gone forever Vote for realDonaldTrump like your life depends on it\n",
      "\n",
      "Removing STOP WORDS with SKLEARN\n",
      "['This', 'is', 'our', 'last', 'stand', 'folks', 'And', 'here’s', 'your', 'last', 'defender', 'If', 'they', 'take', 'him', 'down', 'America', 'is', 'gone', 'forever', 'Vote', 'for', 'realDonaldTrump', 'like', 'your', 'life', 'depends', 'on', 'it']\n"
     ]
    }
   ],
   "source": [
    "# SKLEARN\n",
    "\n",
    "listofwords = df['full_text'][0].split()\n",
    "\n",
    "print(df['full_text'][0])\n",
    "print('')\n",
    "print('Removing STOP WORDS with SKLEARN')\n",
    "print([word for word in listofwords if word is not s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is our last stand folks And here’s your last defender If they take him down America is gone forever Vote for realDonaldTrump like your life depends on it\n",
      "\n",
      "Removing STOP WORDS with SPACY\n",
      "['stand', 'folks', 'defender', 'America', 'gone', 'forever', 'Vote', 'realDonaldTrump', 'like', 'life', 'depends']\n"
     ]
    }
   ],
   "source": [
    "# SPACY\n",
    "\n",
    "doc = sp(df['full_text'][0])\n",
    "tokens = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "print(df['full_text'][0])\n",
    "print('')\n",
    "print('Removing STOP WORDS with SPACY')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPACY removes the most amount of stop words compared to NLTK and SKLEARN. One thing to think about is whether you want these words removed. For me, my focus is tweets from twitter and identifying the topics within the tweet. Therefore for my objective, removing more stop words the better. Therefore, I will use SPACY for applying NLP to Tweets or Social Media content."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
